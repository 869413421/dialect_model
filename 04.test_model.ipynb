{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b67a0a8de5d9e51",
   "metadata": {
    "id": "8b67a0a8de5d9e51"
   },
   "source": [
    "# 模型测试"
   ]
  },
  {
   "cell_type": "code",
   "id": "clEbL9uW8NI9",
   "metadata": {
    "id": "clEbL9uW8NI9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1731547236636,
     "user_tz": -480,
     "elapsed": 26873,
     "user": {
      "displayName": "黄彦铭",
      "userId": "13626583860051468110"
     }
    },
    "outputId": "3656b556-1c5a-47f7-f2ee-be9fbf05e0eb"
   },
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a41ed69ed37d06b5",
   "metadata": {
    "id": "a41ed69ed37d06b5"
   },
   "source": [
    "## 翻译模型测试"
   ]
  },
  {
   "cell_type": "code",
   "id": "c603dd2e2e6447e2",
   "metadata": {
    "id": "c603dd2e2e6447e2"
   },
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# 本地\n",
    "checkpoint = \"./model/t5-02\"\n",
    "\n",
    "# google drive\n",
    "#checkpoint = \"/content/drive/MyDrive/ai-learning/dialect_model/model/t5-02\"\n",
    "\n",
    "# 初始化 tokenizer 和模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# 加载模型，本地文件导入\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2bb26975e527d509",
   "metadata": {
    "id": "2bb26975e527d509"
   },
   "source": [
    "# 输入普通话文本\n",
    "prefix = \"翻译 中文 为 信宜话: \"\n",
    "text = \"你好，你喜欢吃什么小吃？\"\n",
    "input_sentence = prefix + text\n",
    "\n",
    "# 进行转换（普通话到方言）\n",
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation_zh_to_zh\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "print(\"普通话:\", text)\n",
    "print(\"方言:\", translator(input_sentence)[0][\"translation_text\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fa050b3dbb87f07a",
   "metadata": {
    "id": "fa050b3dbb87f07a"
   },
   "source": [
    "## 翻译语料"
   ]
  },
  {
   "cell_type": "code",
   "id": "358ff1b5f96b653d",
   "metadata": {
    "id": "358ff1b5f96b653d",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1731547720525,
     "user_tz": -480,
     "elapsed": 1203,
     "user": {
      "displayName": "黄彦铭",
      "userId": "13626583860051468110"
     }
    }
   },
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "# 本地\n",
    "#checkpoint = \"./model/t5-02\"\n",
    "\n",
    "# google drive\n",
    "checkpoint = \"/content/drive/MyDrive/ai-learning/dialect_model/model/t5-02\"\n",
    "\n",
    "# 初始化 tokenizer 和模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# 加载模型，本地文件导入\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ac0296d2896d402a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac0296d2896d402a",
    "outputId": "bd08433b-1f37-4e82-d013-41fcac5d2bbc"
   },
   "source": [
    "from transformers import pipeline\n",
    "import json, torch, tqdm\n",
    "\n",
    "# 使用 GPU 加速\n",
    "translator = pipeline(\"translation_zh_to_zh\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# 加载 JSONL 文件为 Dataset 格式\n",
    "input_file = \"/content/drive/MyDrive/ai-learning/dialect_model/dataset/train.jsonl\"\n",
    "output_file = \"/content/drive/MyDrive/ai-learning/dialect_model/dataset/chat_trans2.jsonl\"\n",
    "prefix = \"翻译 中文 为 信宜话:\"\n",
    "\n",
    "\n",
    "# 从 JSONL 文件中加载数据\n",
    "def load_jsonl_data(file_path):\n",
    "    json_data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            json_data.append(json.loads(line.strip()))\n",
    "    return json_data\n",
    "\n",
    "\n",
    "# 加载数据并转换为 Hugging Face Dataset\n",
    "data = load_jsonl_data(input_file)\n",
    "data = data[10000:23000]\n",
    "\n",
    "\n",
    "# 将翻译结果分批次写入文件\n",
    "def translate_and_write_batch(batch, f):\n",
    "    with torch.no_grad():\n",
    "        questions = [prefix + q[\"question\"] for q in batch]\n",
    "        answers = [a[\"answer\"] for a in batch]\n",
    "\n",
    "        # 批量翻译问题和答案\n",
    "        translated_questions = translator(questions, max_length=300)\n",
    "        translated_answers = translator(answers, max_length=300)\n",
    "\n",
    "        # 更新原问题和答案为翻译后的文本\n",
    "        for idx, item in enumerate(batch):\n",
    "            item[\"question\"] = translated_questions[idx][\"translation_text\"]\n",
    "            item[\"answer\"] = translated_answers[idx][\"translation_text\"]\n",
    "            # 写入文件\n",
    "            f.write(json.dumps({\"question\": item[\"question\"], \"answer\": item[\"answer\"]}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "# 打开文件准备写入\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for i in tqdm.tqdm(range(0, len(data), 24), total=len(data) // 24):\n",
    "        # 获取当前批次的数据\n",
    "        batch = data[i:i + 24]\n",
    "        translate_and_write_batch(batch, f)\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# “赛博信宜佬”？如何使用transformers实现一个方言对话大模型

## 简介
要说现在科技圈最火的不可置否地应该是AI，而大模型作为AI时代最典型的作品，已经开始逐步渗透到我们日常生活和工作中。文本，视图，音频等领域近两年百花齐放，各种大厂开源的大模型层出不穷高速迭代。得益于开源和技术发展，AI已经不再高不可攀，即便是我们这种普通的互联网用户也能轻松地触摸打造属于自己的大模型。最近使用**transformers**从零样本训练出了一个家乡方言的对话模型，本文就已完成的模型作一次实现思路讲解。

### 效果

#### 音乐相关对话

![](https://raw.githubusercontent.com/869413421/dialect_model/refs/heads/main/image/example5.png)

#### 历史相关对话

![](https://raw.githubusercontent.com/869413421/dialect_model/refs/heads/main/image/example2.png)

#### 介绍城市

![](https://raw.githubusercontent.com/869413421/dialect_model/refs/heads/main/image/example3.png)

#### 经典的9.11和9.9

![](https://raw.githubusercontent.com/869413421/dialect_model/refs/heads/main/image/example4.png)



### 应用场景？

方言大模型具备那些应用场景？

1. **语言保护与传承**：方言大模型可以将面临濒危的方言系统化地记录、存储，并生成文本或语音数据，用于博物馆、档案馆、教育机构等的保存和研究，帮助延续方言的独特文化价值。
2. **智能语音助手**：方言模型可以帮助语音助手支持多种方言，使用户能够使用自己熟悉的方言进行交互，从而在偏远地区或方言区提升语音助手的使用体验，缩小技术鸿沟。
3. **方言翻译**：能够在普通话与各地方言之间实现双向翻译，或者在不同方言之间进行转换，便于不同地区的人交流，尤其在一些语言隔阂较大的地方具有重要意义。
4. **文化内容创作**：方言模型可以帮助创作者创作具有地方特色的文学作品、影视剧对白、方言说唱等，通过生成的内容保留地方语言的风貌。
5. **方言教育**：通过方言大模型生成的口音教学、模拟会话，可以开发线上课程或语言学习App，帮助年轻人学习家乡的方言，增强对家乡文化的认同感。
6. **政务与服务沟通**：在政务服务、医疗、法律等公共服务领域，方言模型能够让服务系统听懂并回应不同方言的需求，降低沟通误解，提升服务质量。
7. **旅游与接待**：在旅游业中，方言模型可以为导游或服务人员提供方言翻译支持，提升游客体验，同时也帮助当地人学习简单的旅游方言交流。



当然，我们这次实现的模型只在文本领域，要实现以上效果，肯定需要巨量的精力和资源，并不是个人可以撬动的。

### 叠甲

- 这并不是一个非常完善的大模型（**缺乏语料！缺乏语料！缺乏语料！**），本文只作为一次技术输出。
- 所有数据都来自各个开源平台
- 文章只讲解实现思路，源码和lora微调的参数都上传至github，源码都是基于**jupyter notebook**，如果你会**transfomers**看懂没什么难度。
- 如果你看不懂？那你可以去学习，正如文前所说，这并不是什么高不可攀的事。


## 信宜？方言大模型
[信宜](https://baike.baidu.com/item/%E4%BF%A1%E5%AE%9C%E5%B8%82/501525)？信宜是我的故乡，一个位于广东粤西地区的小山城。

<pre >信宜市，古称窦州，广东省辖县级市。
由茂名市代管，位于广东省西南部、茂名市北部，东与阳春市相接，南与高州市接壤，西与广西壮族自治区北流市、容县毗邻，北与罗定市、广西壮族自治区岑溪市接壤，土地总面积为3101.7平方公里。
截至2023年6月，信宜市下辖2个街道、18个镇。 
2023年，信宜市常住人口103.33万人。 信宜市是广东省著名侨乡之一、茂名市重点侨乡，先后获得“全国生态保护与建设示范区”“全国重点生态功能区”“中国南玉之都”“中国长寿之乡”“中国慈孝文化之乡”
“全国义务教育发展基本均衡县”“广东省教育强市”等称号。
</pre>
我已经离开这座名不见经传的小城市很多年，以至于日常恍惚时我都会忘记这个城市有什么文化符号。某日在我工作于AI对话那么一瞬间，这些机械刻板吐出来的文字突然让我想再看看那些熟悉的行文。那些困在多年以前跑马灯按键机里，日常不再使用以至于我们记不起的文字，那些近似粤语又带有些许俏皮的的家乡话。“矛day喔”，“系矛咯”，"嘿朝没？",“到嘘定？”，“去荡喔”，“到嘘咧？”，“要几点？”，“到没咯？”，“叼卡奶”，只有深刻地在哪里活过的人，才能感受到这些文字背后的千情万绪。所以那一刻，我想做回忆的拥趸，开始尝试做这一件可能并没有太多实际应用的事情。

## 数据来源

在开始训练之前，我手头上并没有任何一条可以用于方言对话的数据样本，不考虑人工整理（没有精力，一个人做赛博黑奴也有限）。为了获取训练样本，我尝试了以下方式。

- ~~爬虫~~

  目标根据地域关键字去各大社交爬取相关方言对话数据，爬取本地门户网站信息。

- ~~解包微信本地群聊天信息~~

  利用某些手段解包自己微信里和朋友聊天的历史信息。（经过朋友许可，不建议尝试）

  **在尝试以上两种手段收集语料后，非常遗憾地发现。方言的行文方式也逐渐在互联网上消失，取得的数据集少得可怜，全无用处。**



没办法，遇到困难就去解决。没有方言语料，那普通话语料一大堆啊，那我训练一个翻译？所以最终我得出了新的方案。

- 翻译模型

  训练一个翻译模型，让它将中文语料翻译成方言语料。

  - 优点：
    - 不需要人工去整理，所有对话数据都交由机器翻译。
    - 极大减少人工成本。
  - 缺点：
    - 语料资料不高，存在对话不通畅的情况。
    - 缺乏本土化对话场景。



一个人干活，只能用技术去替代人力了。不得不感慨，**大模型的时代，数据才是基石**。如果有大批优质的语料，那么效果不言而喻。

## 翻译模型如何实现

翻译，属于**NLP**任务，还是得益于开源，我们能轻松地在网上找到非常多的实现方案。**transfomers**真是YYDS，我在这里轻松就找到了如何实现一个翻译任务的[文档](https://huggingface.co/learn/nlp-course/chapter7/4?fw=pt)。当然前提是我经常在这里摸鱼，对这个平台非常熟悉，如果你想学习**NLP**和大模型，建议你也对了解**huggingface**和**transfomers**。

### 选择模型基座

要更突出效果，选择预训练的模型非常重要。那种模型更适合翻译任务呢？

其实文档中已经表明了这项任务最适合的是**T5**模型，但是我们也可以选择其他**Encoder-Decoder**结构的模型进行，比如BERT、T5、mBERT、BART，实现方式不尽相同，这里不赘述。

竟然我们使用中文，那么我们就要选择更适合中国宝宝体质的大模型，经过高强度冲浪后一轮筛选，最终选择了[mengzi-t5-base](https://huggingface.co/Langboat/mengzi-t5-base)，孟子T5-base，听名字就知道有多适合了吧。

### 数据预处理
正式训练之前，我们需要给翻译模型一些样本，那这些样本怎么得来呢？
格式如下
<pre>
examples = [
    ("你吃饭了吗？", "你嘿朝没？"),
    ("最近好吗？", "近排点子啊？")，
    ("你不要那么傻", "你矛甘憨")，
]
</pre>
但是这样手工整理，显然不现实，所以我们可以写一个脚本，抽取对话样本里出现频率最高的200个词将这些词翻译成方言关键词，根据方言关键词来进行对普通话文本的替换，最终我们的手工标注任务就缩减为翻译这200个词，是不是一下子就轻松很多了。

基本流程如下：

- 抽取中文对话数据集

- 统计出现最多的词

  - ```python
    import jieba, re
    from collections import Counter
    
    # 将所有标题合并为一个长字符串
    text = " ".join(questions)
    
    # 使用 jieba 进行分词
    words = jieba.lcut(text)
    
    # 使用 Counter 统计词频
    # 去掉标点符号并过滤掉单字词
    filtered_words = [word for word in words if len(word) > 1 and re.match(r'[\u4e00-\u9fff]+', word)]
    
    # 使用 Counter 统计词频
    word_counts = Counter(filtered_words)
    
    # 找出出现次数最多的词
    most_common_words = word_counts.most_common(200)  # 获取出现次数最多的200个词
    
    # 写入CSV
    print("出现次数最多的词：")
    with open('most_common_words.csv', 'w', encoding='utf-8') as f:
        for word, count in most_common_words:
            print(f"{word},{count}")
            f.write(f"{word},{count}\n")
    ```

- 建立词表

- 人工标注

- 简单替换词翻译

  - ```
    def translate(trans_text):
        total_replaced_chars = 0  # 记录替换的字数
    
        for k, v in translate_dict:
            while k in trans_text:
                # 记录替换前后长度的差值（即替换的字数）
                replaced_count = trans_text.count(k) * len(k)
                total_replaced_chars += replaced_count
    
                # 进行替换
                trans_text = trans_text.replace(k, v)
    
        return trans_text, total_replaced_chars
    ```

- 检查翻译结果



虽说这样减少了工作量，但是依然存在问题。
- 简单的文本替换，样本中存在不顺畅的地方。

**其实如果在这个阶段人工介入，把不顺畅的句子改通畅，这样效果可以极大提升，但是个人精力有限（我真不做赛博黑奴）**

到了这个阶段我们其实已经可以对翻译模型进行训练了，要看实际代码可以参考[01-translate_dataset.ipynb](https://github.com/869413421/dialect_model/blob/main/01-translate_dataset.ipynb)。

其实到了这一步，整个实现思路最重要的部分已经阐述出来了，主要是以最少的人力，去生成最大的可用数据，颇有一丝四两拨千斤的感觉。

## 训练对话模型

准备好方言对话语料后，我们就可以正式对我们的对话大模型进行训练了。

至于这部分内容没有什么太多阐述的实现思路，就是正常的加载训练微调模型。

模型我们选择了阿里云最新开源的[Qwen2.5-7B-Instruct]( https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)

这部分的代码可以参考[03.qwen2.5_lora_tuning.ipynb](https://github.com/869413421/dialect_model/blob/main/03.qwen2.5_lora_tuning.ipynb)

这里使用到的技术点有**半精度训练**，**LORA高效参数微调**，如果你不懂这些你可以理解为节省GPU内存和存储资源而出现的一些技术。涉及的代码也不多，自己自行去了解下背后的原理。

### 没有显卡训练怎么办？

其实作者也没有专门训练用的显卡，基本整个流程下来都是使用的云平台。

如果你要学习NLP的话，推荐使用google的[google colab](https://colab.research.google.com/) ，这个平台有免费的GPU资源，每天能用几个小时，搭配谷歌云盘非常方便，我日常学习都在使用这个。但是如果你要训练大模型，这个平台就有点抓襟见肘。因为大模型即便是使用LORA和一些技术手段依然需要很大的内存，这时候只能付费去购买一些云平台的GPU资源进行训练。推荐按时付费的方式，用完就关机，一次可能花费几块钱。



### 技术要点总结

1. **方言大模型的应用场景**：
   - 提到方言模型在语言保护、智能语音助手、方言翻译、文化内容创作、方言教育、公共服务沟通、旅游与接待等领域的广泛应用前景。
2. **数据来源和生成**：
   - **语料短缺**：尝试通过爬虫和微信聊天记录解包方式收集方言数据，但因方言行文逐渐消失，效果不佳。
   - **翻译模型生成方言语料**：通过中文语料生成方言数据。使用“翻译模型”将普通话转为方言，避免人工手工标注。
3. **模型基座选择**：
   - 使用 Hugging Face 的 [mengzi-t5-base](https://huggingface.co/Langboat/mengzi-t5-base) 作为翻译模型基座（适合 Encoder-Decoder 结构）。
   - 选择了阿里云的 [Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct) 作为方言对话模型的基础进行微调。
4. **数据预处理**：
   - **数据清洗和词频统计**：使用 `jieba` 分词与 `Counter` 统计中文对话数据集中高频词汇，并保存到词表文件。
   - **文本替换**：定义替换函数实现普通话到方言的简单词级替换，从而降低人工翻译工作量。
   - **人工标注**：对替换不顺畅的句子进行人工调整，提高翻译质量。
5. **对话模型训练**：
   - 使用 LoRA 微调方法（低秩自适应）对 Qwen2.5 模型进行调优以生成方言对话内容。
   - 代码实现基于 Jupyter Notebook，完整代码发布在 GitHub。



写了那么多让AI帮我总结一下吧。

## 最后

很久没做技术输出，也很久没回家了，不知道家里物事有没有变得飞快。祝福在外打拼的各位好好学习，共同进步，身体健康，万事如意吧。

---

### 参考

1. Hugging Face NLP 课程第七章：[如何实现翻译任务](https://huggingface.co/learn/nlp-course/chapter7/4?fw=pt)
2. Langboat 实验室的孟子 T5 基座模型：[mengzi-t5-base](https://huggingface.co/Langboat/mengzi-t5-base)
3. 阿里云最新开源模型：[Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)
4. [信宜市 - 百度百科](https://baike.baidu.com/item/%E4%BF%A1%E5%AE%9C%E5%B8%82/501525)


{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "927deff85a09470",
   "metadata": {},
   "source": [
    "# 基于Qwen2.5的LoRA微调\n",
    "\n",
    "参考以下教程实现\n",
    "\n",
    "[Qwen2.5-7B-Instruct Lora 微调](https://github.com/datawhalechina/self-llm?tab=readme-ov-file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41ed69ed37d06b5",
   "metadata": {},
   "source": [
    "## Step1 导入相关包"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = '/root/autodl-tmp/huggingface'\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = '/root/autodl-tmp/huggingface'"
   ],
   "id": "cce949a3555a31c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d0180aba-4ad0-41ce-98f6-8c0b6a60d363",
   "metadata": {},
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3dfa91bcecdb0dc0",
   "metadata": {},
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5abc34dbf4625321",
   "metadata": {},
   "source": [
    "## Step2 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "id": "836435e97d58c799",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "# 数据集路径\n",
    "input_file = \"./dataset/chat_trans.jsonl\"\n",
    "\n",
    "\n",
    "def load_jsonl_data(file_path):\n",
    "    json_data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            json_data.append(json.loads(line.strip()))\n",
    "    return json_data\n",
    "\n",
    "\n",
    "# 将数据加载为 Dataset 格式\n",
    "data = load_jsonl_data(input_file)\n",
    "dataset = Dataset.from_list(data)\n",
    "dataset"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6172b0fb9209ff87",
   "metadata": {},
   "source": [
    "## Step3 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "id": "efb72e732b5125e",
   "metadata": {},
   "source": [
    "# 这里使用的是AUTODL上的镜像，直接加载镜像里的模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\", use_fast=False, trust_remote_code=True)\n",
    "#tokenizer = AutoTokenizer.from_pretrained('/root/autodl-tmp/qwen/Qwen2.5-7B-Instruct/', use_fast=False, trust_remote_code=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "995a9aa0-50af-48e2-94f4-42f2ab5acf09",
   "metadata": {},
   "source": [
    "def process_func(example):\n",
    "    MAX_LENGTH = 384\n",
    "    input_ids_batch, attention_mask_batch, labels_batch = [], [], []\n",
    "\n",
    "    for idx, question in enumerate(example['question']):\n",
    "        instruction = tokenizer(f\"<|im_start|>system\\n宜家你系讲信宜话个智能助手--阿信<|im_end|>\\n<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "                                add_special_tokens=False)\n",
    "        response = tokenizer(f\"{example['answer'][idx]}\", add_special_tokens=False)\n",
    "        input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "        attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]\n",
    "        labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "\n",
    "        # 截断操作\n",
    "        if len(input_ids) > MAX_LENGTH:\n",
    "            input_ids = input_ids[:MAX_LENGTH]\n",
    "            attention_mask = attention_mask[:MAX_LENGTH]\n",
    "            labels = labels[:MAX_LENGTH]\n",
    "\n",
    "        # 添加到批量列表\n",
    "        input_ids_batch.append(input_ids)\n",
    "        attention_mask_batch.append(attention_mask)\n",
    "        labels_batch.append(labels)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids_batch,\n",
    "        \"attention_mask\": attention_mask_batch,\n",
    "        \"labels\": labels_batch\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5fdb536357d1494e",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "tokenized_dataset = dataset.map(process_func, batched=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3c15d6e22eb395d4",
   "metadata": {},
   "source": [
    "# 半精度加载模型，减少内存使用\n",
    "# model = AutoModelForCausalLM.from_pretrained('/root/autodl-tmp/qwen/Qwen2.5-7B-Instruct/', device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\", device_map=\"auto\", torch_dtype=torch.bfloat16)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "80537fd354805f57",
   "metadata": {},
   "source": [
    "model.enable_input_require_grads()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c453f269cfd5569f",
   "metadata": {},
   "source": [
    "model.dtype"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "daf07f31df572b73",
   "metadata": {},
   "source": [
    "## Step5 定义lora参数"
   ]
  },
  {
   "cell_type": "code",
   "id": "57897e5600760397",
   "metadata": {},
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    inference_mode=False,  # 训练模式\n",
    "    r=8,  # Lora 秩\n",
    "    lora_alpha=32,  # Lora alaph，具体作用参见 Lora 原理\n",
    "    lora_dropout=0.1  # Dropout 比例\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1f6505c14c7a3d0",
   "metadata": {},
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "# # 应用 LoRA 配置到模型\n",
    "model = get_peft_model(model, config)\n",
    "config"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "20848b29773adeff",
   "metadata": {},
   "source": [
    "# 查看训练参数\n",
    "model.print_trainable_parameters()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "54ecf0e6f71aebf6",
   "metadata": {},
   "source": [
    "## Step6 配置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "id": "d478eef4bc90d759",
   "metadata": {},
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./output/Qwen2.5_instruct_lora\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=100,\n",
    "    learning_rate=1e-4,\n",
    "    save_on_each_node=True,\n",
    "    gradient_checkpointing=True\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e3ccfc5833877867",
   "metadata": {},
   "source": [
    "## Step7 创建Trainer"
   ]
  },
  {
   "cell_type": "code",
   "id": "ce45b53cd5dc7847",
   "metadata": {},
   "source": [
    "from transformers import DataCollatorForSeq2Seq, Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a3b198590b86f651",
   "metadata": {},
   "source": [
    "## Step8 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "id": "25862bfafe18e098",
   "metadata": {},
   "source": [
    "trainer.train()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b283a654ba3b01dc",
   "metadata": {},
   "source": [
    "## Step9 评估模型"
   ]
  },
  {
   "cell_type": "code",
   "id": "95b248308802b1ff",
   "metadata": {},
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "mode_path = '/root/autodl-tmp/qwen/Qwen2.5-7B-Instruct/'\n",
    "lora_path = './output/Qwen2.5_instruct_lora/checkpoint-10'  # 这里改称你的 lora 输出对应 checkpoint 地址\n",
    "\n",
    "# 加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(mode_path, trust_remote_code=True)\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(mode_path, device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True).eval()\n",
    "\n",
    "# 加载lora权重\n",
    "model = PeftModel.from_pretrained(model, model_id=lora_path)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8ebb49cf48bb4803",
   "metadata": {},
   "source": [
    "prompt = \"你是谁？\"\n",
    "inputs = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": \"宜家你系讲信宜话个助手--阿信\"}, {\"role\": \"user\", \"content\": prompt}],\n",
    "                                       add_generation_prompt=True,\n",
    "                                       tokenize=True,\n",
    "                                       return_tensors=\"pt\",\n",
    "                                       return_dict=True\n",
    "                                       ).to('cuda')\n",
    "\n",
    "gen_kwargs = {\"max_length\": 2500, \"do_sample\": True, \"top_k\": 1}\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, **gen_kwargs)\n",
    "    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7acfa105b6fd83a5",
   "metadata": {},
   "source": [
    "## Step10 保存模型"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

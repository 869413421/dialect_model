{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.9981285090455394,
  "eval_steps": 500,
  "global_step": 2403,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.012476606363069246,
      "grad_norm": 3.0625,
      "learning_rate": 9.958385351643778e-05,
      "loss": 3.3219,
      "step": 10
    },
    {
      "epoch": 0.024953212726138492,
      "grad_norm": 3.765625,
      "learning_rate": 9.916770703287557e-05,
      "loss": 2.2312,
      "step": 20
    },
    {
      "epoch": 0.037429819089207735,
      "grad_norm": 3.0,
      "learning_rate": 9.875156054931336e-05,
      "loss": 1.8905,
      "step": 30
    },
    {
      "epoch": 0.049906425452276984,
      "grad_norm": 2.515625,
      "learning_rate": 9.833541406575115e-05,
      "loss": 1.6969,
      "step": 40
    },
    {
      "epoch": 0.06238303181534623,
      "grad_norm": 2.84375,
      "learning_rate": 9.791926758218893e-05,
      "loss": 1.6502,
      "step": 50
    },
    {
      "epoch": 0.07485963817841547,
      "grad_norm": 3.21875,
      "learning_rate": 9.750312109862672e-05,
      "loss": 1.6118,
      "step": 60
    },
    {
      "epoch": 0.08733624454148471,
      "grad_norm": 4.46875,
      "learning_rate": 9.708697461506451e-05,
      "loss": 1.5794,
      "step": 70
    },
    {
      "epoch": 0.09981285090455397,
      "grad_norm": 2.765625,
      "learning_rate": 9.667082813150229e-05,
      "loss": 1.4449,
      "step": 80
    },
    {
      "epoch": 0.11228945726762321,
      "grad_norm": 2.828125,
      "learning_rate": 9.625468164794008e-05,
      "loss": 1.4482,
      "step": 90
    },
    {
      "epoch": 0.12476606363069245,
      "grad_norm": 3.1875,
      "learning_rate": 9.583853516437786e-05,
      "loss": 1.4366,
      "step": 100
    },
    {
      "epoch": 0.1372426699937617,
      "grad_norm": 2.5,
      "learning_rate": 9.542238868081565e-05,
      "loss": 1.498,
      "step": 110
    },
    {
      "epoch": 0.14971927635683094,
      "grad_norm": 3.1875,
      "learning_rate": 9.500624219725344e-05,
      "loss": 1.4554,
      "step": 120
    },
    {
      "epoch": 0.16219588271990018,
      "grad_norm": 2.53125,
      "learning_rate": 9.459009571369123e-05,
      "loss": 1.376,
      "step": 130
    },
    {
      "epoch": 0.17467248908296942,
      "grad_norm": 2.984375,
      "learning_rate": 9.417394923012901e-05,
      "loss": 1.4069,
      "step": 140
    },
    {
      "epoch": 0.18714909544603867,
      "grad_norm": 3.171875,
      "learning_rate": 9.375780274656679e-05,
      "loss": 1.3816,
      "step": 150
    },
    {
      "epoch": 0.19962570180910794,
      "grad_norm": 3.546875,
      "learning_rate": 9.334165626300458e-05,
      "loss": 1.3025,
      "step": 160
    },
    {
      "epoch": 0.21210230817217718,
      "grad_norm": 2.5,
      "learning_rate": 9.292550977944237e-05,
      "loss": 1.2921,
      "step": 170
    },
    {
      "epoch": 0.22457891453524642,
      "grad_norm": 3.09375,
      "learning_rate": 9.250936329588016e-05,
      "loss": 1.2633,
      "step": 180
    },
    {
      "epoch": 0.23705552089831566,
      "grad_norm": 3.125,
      "learning_rate": 9.209321681231794e-05,
      "loss": 1.3228,
      "step": 190
    },
    {
      "epoch": 0.2495321272613849,
      "grad_norm": 4.1875,
      "learning_rate": 9.167707032875571e-05,
      "loss": 1.2979,
      "step": 200
    },
    {
      "epoch": 0.26200873362445415,
      "grad_norm": 2.703125,
      "learning_rate": 9.126092384519352e-05,
      "loss": 1.3154,
      "step": 210
    },
    {
      "epoch": 0.2744853399875234,
      "grad_norm": 2.875,
      "learning_rate": 9.08447773616313e-05,
      "loss": 1.3121,
      "step": 220
    },
    {
      "epoch": 0.28696194635059263,
      "grad_norm": 3.53125,
      "learning_rate": 9.042863087806909e-05,
      "loss": 1.2902,
      "step": 230
    },
    {
      "epoch": 0.2994385527136619,
      "grad_norm": 3.578125,
      "learning_rate": 9.001248439450686e-05,
      "loss": 1.2509,
      "step": 240
    },
    {
      "epoch": 0.3119151590767311,
      "grad_norm": 2.640625,
      "learning_rate": 8.959633791094466e-05,
      "loss": 1.2571,
      "step": 250
    },
    {
      "epoch": 0.32439176543980036,
      "grad_norm": 2.765625,
      "learning_rate": 8.918019142738245e-05,
      "loss": 1.2286,
      "step": 260
    },
    {
      "epoch": 0.3368683718028696,
      "grad_norm": 2.90625,
      "learning_rate": 8.876404494382022e-05,
      "loss": 1.229,
      "step": 270
    },
    {
      "epoch": 0.34934497816593885,
      "grad_norm": 3.0,
      "learning_rate": 8.834789846025801e-05,
      "loss": 1.2468,
      "step": 280
    },
    {
      "epoch": 0.3618215845290081,
      "grad_norm": 4.03125,
      "learning_rate": 8.793175197669579e-05,
      "loss": 1.3177,
      "step": 290
    },
    {
      "epoch": 0.37429819089207733,
      "grad_norm": 3.1875,
      "learning_rate": 8.75156054931336e-05,
      "loss": 1.2224,
      "step": 300
    },
    {
      "epoch": 0.3867747972551466,
      "grad_norm": 2.671875,
      "learning_rate": 8.709945900957137e-05,
      "loss": 1.2653,
      "step": 310
    },
    {
      "epoch": 0.39925140361821587,
      "grad_norm": 3.65625,
      "learning_rate": 8.668331252600916e-05,
      "loss": 1.2216,
      "step": 320
    },
    {
      "epoch": 0.4117280099812851,
      "grad_norm": 3.609375,
      "learning_rate": 8.626716604244694e-05,
      "loss": 1.2263,
      "step": 330
    },
    {
      "epoch": 0.42420461634435436,
      "grad_norm": 3.03125,
      "learning_rate": 8.585101955888473e-05,
      "loss": 1.1824,
      "step": 340
    },
    {
      "epoch": 0.4366812227074236,
      "grad_norm": 3.65625,
      "learning_rate": 8.543487307532252e-05,
      "loss": 1.2,
      "step": 350
    },
    {
      "epoch": 0.44915782907049284,
      "grad_norm": 2.484375,
      "learning_rate": 8.50187265917603e-05,
      "loss": 1.2619,
      "step": 360
    },
    {
      "epoch": 0.4616344354335621,
      "grad_norm": 3.3125,
      "learning_rate": 8.460258010819809e-05,
      "loss": 1.2582,
      "step": 370
    },
    {
      "epoch": 0.4741110417966313,
      "grad_norm": 3.15625,
      "learning_rate": 8.418643362463587e-05,
      "loss": 1.1907,
      "step": 380
    },
    {
      "epoch": 0.48658764815970057,
      "grad_norm": 3.421875,
      "learning_rate": 8.377028714107366e-05,
      "loss": 1.1788,
      "step": 390
    },
    {
      "epoch": 0.4990642545227698,
      "grad_norm": 3.765625,
      "learning_rate": 8.335414065751145e-05,
      "loss": 1.2186,
      "step": 400
    },
    {
      "epoch": 0.511540860885839,
      "grad_norm": 3.71875,
      "learning_rate": 8.293799417394923e-05,
      "loss": 1.1858,
      "step": 410
    },
    {
      "epoch": 0.5240174672489083,
      "grad_norm": 2.921875,
      "learning_rate": 8.252184769038702e-05,
      "loss": 1.2212,
      "step": 420
    },
    {
      "epoch": 0.5364940736119775,
      "grad_norm": 2.65625,
      "learning_rate": 8.21057012068248e-05,
      "loss": 1.1986,
      "step": 430
    },
    {
      "epoch": 0.5489706799750468,
      "grad_norm": 3.515625,
      "learning_rate": 8.16895547232626e-05,
      "loss": 1.3066,
      "step": 440
    },
    {
      "epoch": 0.561447286338116,
      "grad_norm": 2.9375,
      "learning_rate": 8.127340823970038e-05,
      "loss": 1.1155,
      "step": 450
    },
    {
      "epoch": 0.5739238927011853,
      "grad_norm": 2.28125,
      "learning_rate": 8.085726175613817e-05,
      "loss": 1.2436,
      "step": 460
    },
    {
      "epoch": 0.5864004990642545,
      "grad_norm": 3.09375,
      "learning_rate": 8.044111527257595e-05,
      "loss": 1.1815,
      "step": 470
    },
    {
      "epoch": 0.5988771054273238,
      "grad_norm": 2.5625,
      "learning_rate": 8.002496878901374e-05,
      "loss": 1.2291,
      "step": 480
    },
    {
      "epoch": 0.611353711790393,
      "grad_norm": 3.796875,
      "learning_rate": 7.960882230545153e-05,
      "loss": 1.1616,
      "step": 490
    },
    {
      "epoch": 0.6238303181534622,
      "grad_norm": 3.4375,
      "learning_rate": 7.91926758218893e-05,
      "loss": 1.2165,
      "step": 500
    },
    {
      "epoch": 0.6363069245165315,
      "grad_norm": 2.625,
      "learning_rate": 7.87765293383271e-05,
      "loss": 1.1458,
      "step": 510
    },
    {
      "epoch": 0.6487835308796007,
      "grad_norm": 2.765625,
      "learning_rate": 7.836038285476487e-05,
      "loss": 1.1086,
      "step": 520
    },
    {
      "epoch": 0.66126013724267,
      "grad_norm": 4.03125,
      "learning_rate": 7.794423637120268e-05,
      "loss": 1.1733,
      "step": 530
    },
    {
      "epoch": 0.6737367436057392,
      "grad_norm": 2.375,
      "learning_rate": 7.752808988764046e-05,
      "loss": 1.1881,
      "step": 540
    },
    {
      "epoch": 0.6862133499688085,
      "grad_norm": 3.6875,
      "learning_rate": 7.711194340407823e-05,
      "loss": 1.162,
      "step": 550
    },
    {
      "epoch": 0.6986899563318777,
      "grad_norm": 3.09375,
      "learning_rate": 7.669579692051602e-05,
      "loss": 1.0943,
      "step": 560
    },
    {
      "epoch": 0.7111665626949469,
      "grad_norm": 2.921875,
      "learning_rate": 7.627965043695382e-05,
      "loss": 1.147,
      "step": 570
    },
    {
      "epoch": 0.7236431690580162,
      "grad_norm": 2.875,
      "learning_rate": 7.58635039533916e-05,
      "loss": 1.0856,
      "step": 580
    },
    {
      "epoch": 0.7361197754210854,
      "grad_norm": 2.78125,
      "learning_rate": 7.544735746982938e-05,
      "loss": 1.0838,
      "step": 590
    },
    {
      "epoch": 0.7485963817841547,
      "grad_norm": 3.0,
      "learning_rate": 7.503121098626716e-05,
      "loss": 1.0462,
      "step": 600
    },
    {
      "epoch": 0.7610729881472239,
      "grad_norm": 5.0,
      "learning_rate": 7.461506450270495e-05,
      "loss": 1.1834,
      "step": 610
    },
    {
      "epoch": 0.7735495945102931,
      "grad_norm": 3.171875,
      "learning_rate": 7.419891801914274e-05,
      "loss": 1.1065,
      "step": 620
    },
    {
      "epoch": 0.7860262008733624,
      "grad_norm": 2.6875,
      "learning_rate": 7.378277153558053e-05,
      "loss": 1.1038,
      "step": 630
    },
    {
      "epoch": 0.7985028072364317,
      "grad_norm": 3.390625,
      "learning_rate": 7.336662505201831e-05,
      "loss": 1.1249,
      "step": 640
    },
    {
      "epoch": 0.810979413599501,
      "grad_norm": 3.203125,
      "learning_rate": 7.29504785684561e-05,
      "loss": 1.2031,
      "step": 650
    },
    {
      "epoch": 0.8234560199625702,
      "grad_norm": 3.4375,
      "learning_rate": 7.253433208489388e-05,
      "loss": 1.0942,
      "step": 660
    },
    {
      "epoch": 0.8359326263256395,
      "grad_norm": 2.609375,
      "learning_rate": 7.211818560133167e-05,
      "loss": 1.1392,
      "step": 670
    },
    {
      "epoch": 0.8484092326887087,
      "grad_norm": 2.578125,
      "learning_rate": 7.170203911776946e-05,
      "loss": 1.0629,
      "step": 680
    },
    {
      "epoch": 0.860885839051778,
      "grad_norm": 3.921875,
      "learning_rate": 7.128589263420724e-05,
      "loss": 1.1479,
      "step": 690
    },
    {
      "epoch": 0.8733624454148472,
      "grad_norm": 3.078125,
      "learning_rate": 7.086974615064503e-05,
      "loss": 1.0985,
      "step": 700
    },
    {
      "epoch": 0.8858390517779164,
      "grad_norm": 2.859375,
      "learning_rate": 7.045359966708282e-05,
      "loss": 1.0877,
      "step": 710
    },
    {
      "epoch": 0.8983156581409857,
      "grad_norm": 2.953125,
      "learning_rate": 7.003745318352061e-05,
      "loss": 1.0879,
      "step": 720
    },
    {
      "epoch": 0.9107922645040549,
      "grad_norm": 2.6875,
      "learning_rate": 6.962130669995839e-05,
      "loss": 1.0423,
      "step": 730
    },
    {
      "epoch": 0.9232688708671242,
      "grad_norm": 3.234375,
      "learning_rate": 6.920516021639617e-05,
      "loss": 1.1279,
      "step": 740
    },
    {
      "epoch": 0.9357454772301934,
      "grad_norm": 2.75,
      "learning_rate": 6.878901373283396e-05,
      "loss": 1.0956,
      "step": 750
    },
    {
      "epoch": 0.9482220835932627,
      "grad_norm": 3.78125,
      "learning_rate": 6.837286724927175e-05,
      "loss": 1.0897,
      "step": 760
    },
    {
      "epoch": 0.9606986899563319,
      "grad_norm": 2.46875,
      "learning_rate": 6.795672076570954e-05,
      "loss": 1.1692,
      "step": 770
    },
    {
      "epoch": 0.9731752963194011,
      "grad_norm": 2.390625,
      "learning_rate": 6.754057428214732e-05,
      "loss": 1.0992,
      "step": 780
    },
    {
      "epoch": 0.9856519026824704,
      "grad_norm": 2.6875,
      "learning_rate": 6.71244277985851e-05,
      "loss": 1.1279,
      "step": 790
    },
    {
      "epoch": 0.9981285090455396,
      "grad_norm": 2.875,
      "learning_rate": 6.67082813150229e-05,
      "loss": 1.0507,
      "step": 800
    },
    {
      "epoch": 1.0106051154086089,
      "grad_norm": 3.046875,
      "learning_rate": 6.629213483146067e-05,
      "loss": 0.948,
      "step": 810
    },
    {
      "epoch": 1.023081721771678,
      "grad_norm": 2.875,
      "learning_rate": 6.587598834789847e-05,
      "loss": 0.9312,
      "step": 820
    },
    {
      "epoch": 1.0355583281347474,
      "grad_norm": 3.109375,
      "learning_rate": 6.545984186433624e-05,
      "loss": 0.8947,
      "step": 830
    },
    {
      "epoch": 1.0480349344978166,
      "grad_norm": 3.359375,
      "learning_rate": 6.504369538077403e-05,
      "loss": 0.9023,
      "step": 840
    },
    {
      "epoch": 1.0605115408608858,
      "grad_norm": 3.859375,
      "learning_rate": 6.462754889721182e-05,
      "loss": 0.9282,
      "step": 850
    },
    {
      "epoch": 1.072988147223955,
      "grad_norm": 3.09375,
      "learning_rate": 6.421140241364962e-05,
      "loss": 0.8771,
      "step": 860
    },
    {
      "epoch": 1.0854647535870243,
      "grad_norm": 3.0625,
      "learning_rate": 6.379525593008739e-05,
      "loss": 0.9136,
      "step": 870
    },
    {
      "epoch": 1.0979413599500936,
      "grad_norm": 2.890625,
      "learning_rate": 6.337910944652517e-05,
      "loss": 0.8306,
      "step": 880
    },
    {
      "epoch": 1.1104179663131628,
      "grad_norm": 3.84375,
      "learning_rate": 6.296296296296296e-05,
      "loss": 0.8854,
      "step": 890
    },
    {
      "epoch": 1.122894572676232,
      "grad_norm": 2.921875,
      "learning_rate": 6.254681647940075e-05,
      "loss": 0.9131,
      "step": 900
    },
    {
      "epoch": 1.1353711790393013,
      "grad_norm": 4.65625,
      "learning_rate": 6.213066999583854e-05,
      "loss": 0.9023,
      "step": 910
    },
    {
      "epoch": 1.1478477854023705,
      "grad_norm": 3.15625,
      "learning_rate": 6.171452351227632e-05,
      "loss": 0.8655,
      "step": 920
    },
    {
      "epoch": 1.1603243917654398,
      "grad_norm": 3.375,
      "learning_rate": 6.129837702871411e-05,
      "loss": 0.8652,
      "step": 930
    },
    {
      "epoch": 1.172800998128509,
      "grad_norm": 3.5,
      "learning_rate": 6.0882230545151896e-05,
      "loss": 0.8841,
      "step": 940
    },
    {
      "epoch": 1.1852776044915783,
      "grad_norm": 3.375,
      "learning_rate": 6.046608406158968e-05,
      "loss": 0.916,
      "step": 950
    },
    {
      "epoch": 1.1977542108546475,
      "grad_norm": 3.734375,
      "learning_rate": 6.004993757802747e-05,
      "loss": 0.9422,
      "step": 960
    },
    {
      "epoch": 1.2102308172177167,
      "grad_norm": 3.078125,
      "learning_rate": 5.9633791094465255e-05,
      "loss": 0.8601,
      "step": 970
    },
    {
      "epoch": 1.222707423580786,
      "grad_norm": 2.796875,
      "learning_rate": 5.9217644610903046e-05,
      "loss": 0.8896,
      "step": 980
    },
    {
      "epoch": 1.2351840299438552,
      "grad_norm": 3.53125,
      "learning_rate": 5.880149812734082e-05,
      "loss": 0.9354,
      "step": 990
    },
    {
      "epoch": 1.2476606363069245,
      "grad_norm": 3.625,
      "learning_rate": 5.838535164377861e-05,
      "loss": 0.9388,
      "step": 1000
    },
    {
      "epoch": 1.2601372426699937,
      "grad_norm": 3.796875,
      "learning_rate": 5.79692051602164e-05,
      "loss": 0.8313,
      "step": 1010
    },
    {
      "epoch": 1.272613849033063,
      "grad_norm": 2.828125,
      "learning_rate": 5.755305867665418e-05,
      "loss": 0.8538,
      "step": 1020
    },
    {
      "epoch": 1.2850904553961322,
      "grad_norm": 3.546875,
      "learning_rate": 5.713691219309197e-05,
      "loss": 0.8292,
      "step": 1030
    },
    {
      "epoch": 1.2975670617592014,
      "grad_norm": 3.1875,
      "learning_rate": 5.672076570952976e-05,
      "loss": 0.9565,
      "step": 1040
    },
    {
      "epoch": 1.3100436681222707,
      "grad_norm": 3.4375,
      "learning_rate": 5.630461922596755e-05,
      "loss": 0.9415,
      "step": 1050
    },
    {
      "epoch": 1.32252027448534,
      "grad_norm": 4.375,
      "learning_rate": 5.588847274240533e-05,
      "loss": 0.9231,
      "step": 1060
    },
    {
      "epoch": 1.3349968808484092,
      "grad_norm": 3.15625,
      "learning_rate": 5.547232625884311e-05,
      "loss": 0.8385,
      "step": 1070
    },
    {
      "epoch": 1.3474734872114784,
      "grad_norm": 4.5,
      "learning_rate": 5.50561797752809e-05,
      "loss": 0.8986,
      "step": 1080
    },
    {
      "epoch": 1.3599500935745477,
      "grad_norm": 3.375,
      "learning_rate": 5.4640033291718685e-05,
      "loss": 0.8741,
      "step": 1090
    },
    {
      "epoch": 1.372426699937617,
      "grad_norm": 3.53125,
      "learning_rate": 5.4223886808156476e-05,
      "loss": 0.8788,
      "step": 1100
    },
    {
      "epoch": 1.3849033063006861,
      "grad_norm": 3.625,
      "learning_rate": 5.380774032459426e-05,
      "loss": 0.9284,
      "step": 1110
    },
    {
      "epoch": 1.3973799126637554,
      "grad_norm": 3.109375,
      "learning_rate": 5.339159384103205e-05,
      "loss": 0.8516,
      "step": 1120
    },
    {
      "epoch": 1.4098565190268246,
      "grad_norm": 3.640625,
      "learning_rate": 5.2975447357469835e-05,
      "loss": 0.8245,
      "step": 1130
    },
    {
      "epoch": 1.4223331253898939,
      "grad_norm": 3.390625,
      "learning_rate": 5.255930087390761e-05,
      "loss": 0.832,
      "step": 1140
    },
    {
      "epoch": 1.4348097317529631,
      "grad_norm": 4.09375,
      "learning_rate": 5.21431543903454e-05,
      "loss": 0.9132,
      "step": 1150
    },
    {
      "epoch": 1.4472863381160324,
      "grad_norm": 3.703125,
      "learning_rate": 5.172700790678319e-05,
      "loss": 0.8363,
      "step": 1160
    },
    {
      "epoch": 1.4597629444791016,
      "grad_norm": 3.4375,
      "learning_rate": 5.131086142322098e-05,
      "loss": 0.8187,
      "step": 1170
    },
    {
      "epoch": 1.472239550842171,
      "grad_norm": 3.921875,
      "learning_rate": 5.089471493965876e-05,
      "loss": 0.8347,
      "step": 1180
    },
    {
      "epoch": 1.48471615720524,
      "grad_norm": 2.734375,
      "learning_rate": 5.047856845609655e-05,
      "loss": 0.8053,
      "step": 1190
    },
    {
      "epoch": 1.4971927635683095,
      "grad_norm": 3.34375,
      "learning_rate": 5.006242197253434e-05,
      "loss": 0.8802,
      "step": 1200
    },
    {
      "epoch": 1.5096693699313786,
      "grad_norm": 3.921875,
      "learning_rate": 4.964627548897212e-05,
      "loss": 0.8532,
      "step": 1210
    },
    {
      "epoch": 1.522145976294448,
      "grad_norm": 3.859375,
      "learning_rate": 4.9230129005409905e-05,
      "loss": 0.8458,
      "step": 1220
    },
    {
      "epoch": 1.534622582657517,
      "grad_norm": 3.65625,
      "learning_rate": 4.8813982521847696e-05,
      "loss": 0.8446,
      "step": 1230
    },
    {
      "epoch": 1.5470991890205865,
      "grad_norm": 3.34375,
      "learning_rate": 4.8397836038285474e-05,
      "loss": 0.8317,
      "step": 1240
    },
    {
      "epoch": 1.5595757953836555,
      "grad_norm": 3.65625,
      "learning_rate": 4.7981689554723265e-05,
      "loss": 0.9782,
      "step": 1250
    },
    {
      "epoch": 1.572052401746725,
      "grad_norm": 3.234375,
      "learning_rate": 4.756554307116105e-05,
      "loss": 0.8139,
      "step": 1260
    },
    {
      "epoch": 1.584529008109794,
      "grad_norm": 3.46875,
      "learning_rate": 4.714939658759884e-05,
      "loss": 0.8531,
      "step": 1270
    },
    {
      "epoch": 1.5970056144728635,
      "grad_norm": 4.25,
      "learning_rate": 4.6733250104036624e-05,
      "loss": 0.8806,
      "step": 1280
    },
    {
      "epoch": 1.6094822208359325,
      "grad_norm": 3.4375,
      "learning_rate": 4.6317103620474415e-05,
      "loss": 0.82,
      "step": 1290
    },
    {
      "epoch": 1.621958827199002,
      "grad_norm": 3.421875,
      "learning_rate": 4.590095713691219e-05,
      "loss": 0.8261,
      "step": 1300
    },
    {
      "epoch": 1.634435433562071,
      "grad_norm": 3.25,
      "learning_rate": 4.548481065334998e-05,
      "loss": 0.82,
      "step": 1310
    },
    {
      "epoch": 1.6469120399251405,
      "grad_norm": 3.59375,
      "learning_rate": 4.506866416978777e-05,
      "loss": 0.857,
      "step": 1320
    },
    {
      "epoch": 1.6593886462882095,
      "grad_norm": 3.375,
      "learning_rate": 4.465251768622555e-05,
      "loss": 0.8075,
      "step": 1330
    },
    {
      "epoch": 1.671865252651279,
      "grad_norm": 3.03125,
      "learning_rate": 4.423637120266334e-05,
      "loss": 0.8483,
      "step": 1340
    },
    {
      "epoch": 1.684341859014348,
      "grad_norm": 3.015625,
      "learning_rate": 4.3820224719101126e-05,
      "loss": 0.9207,
      "step": 1350
    },
    {
      "epoch": 1.6968184653774174,
      "grad_norm": 3.734375,
      "learning_rate": 4.340407823553892e-05,
      "loss": 0.8334,
      "step": 1360
    },
    {
      "epoch": 1.7092950717404864,
      "grad_norm": 3.34375,
      "learning_rate": 4.2987931751976694e-05,
      "loss": 0.8516,
      "step": 1370
    },
    {
      "epoch": 1.721771678103556,
      "grad_norm": 3.15625,
      "learning_rate": 4.2571785268414485e-05,
      "loss": 0.8972,
      "step": 1380
    },
    {
      "epoch": 1.734248284466625,
      "grad_norm": 3.125,
      "learning_rate": 4.215563878485227e-05,
      "loss": 0.8965,
      "step": 1390
    },
    {
      "epoch": 1.7467248908296944,
      "grad_norm": 3.4375,
      "learning_rate": 4.1739492301290054e-05,
      "loss": 0.7824,
      "step": 1400
    },
    {
      "epoch": 1.7592014971927634,
      "grad_norm": 3.21875,
      "learning_rate": 4.1323345817727844e-05,
      "loss": 0.9138,
      "step": 1410
    },
    {
      "epoch": 1.7716781035558329,
      "grad_norm": 3.46875,
      "learning_rate": 4.090719933416563e-05,
      "loss": 0.8641,
      "step": 1420
    },
    {
      "epoch": 1.784154709918902,
      "grad_norm": 3.578125,
      "learning_rate": 4.049105285060342e-05,
      "loss": 0.8661,
      "step": 1430
    },
    {
      "epoch": 1.7966313162819714,
      "grad_norm": 4.03125,
      "learning_rate": 4.00749063670412e-05,
      "loss": 0.84,
      "step": 1440
    },
    {
      "epoch": 1.8091079226450404,
      "grad_norm": 2.625,
      "learning_rate": 3.965875988347899e-05,
      "loss": 0.8329,
      "step": 1450
    },
    {
      "epoch": 1.8215845290081099,
      "grad_norm": 2.546875,
      "learning_rate": 3.924261339991677e-05,
      "loss": 0.8289,
      "step": 1460
    },
    {
      "epoch": 1.8340611353711789,
      "grad_norm": 3.953125,
      "learning_rate": 3.8826466916354556e-05,
      "loss": 0.8634,
      "step": 1470
    },
    {
      "epoch": 1.8465377417342483,
      "grad_norm": 3.3125,
      "learning_rate": 3.841032043279235e-05,
      "loss": 0.8559,
      "step": 1480
    },
    {
      "epoch": 1.8590143480973176,
      "grad_norm": 3.09375,
      "learning_rate": 3.799417394923013e-05,
      "loss": 0.8915,
      "step": 1490
    },
    {
      "epoch": 1.8714909544603868,
      "grad_norm": 2.78125,
      "learning_rate": 3.7578027465667915e-05,
      "loss": 0.8043,
      "step": 1500
    },
    {
      "epoch": 1.883967560823456,
      "grad_norm": 3.109375,
      "learning_rate": 3.71618809821057e-05,
      "loss": 0.9178,
      "step": 1510
    },
    {
      "epoch": 1.8964441671865253,
      "grad_norm": 3.4375,
      "learning_rate": 3.674573449854349e-05,
      "loss": 0.7987,
      "step": 1520
    },
    {
      "epoch": 1.9089207735495946,
      "grad_norm": 3.75,
      "learning_rate": 3.6329588014981274e-05,
      "loss": 0.8754,
      "step": 1530
    },
    {
      "epoch": 1.9213973799126638,
      "grad_norm": 3.71875,
      "learning_rate": 3.5913441531419065e-05,
      "loss": 0.9052,
      "step": 1540
    },
    {
      "epoch": 1.933873986275733,
      "grad_norm": 3.5625,
      "learning_rate": 3.549729504785685e-05,
      "loss": 0.8285,
      "step": 1550
    },
    {
      "epoch": 1.9463505926388023,
      "grad_norm": 2.890625,
      "learning_rate": 3.5081148564294633e-05,
      "loss": 0.7385,
      "step": 1560
    },
    {
      "epoch": 1.9588271990018715,
      "grad_norm": 3.140625,
      "learning_rate": 3.466500208073242e-05,
      "loss": 0.8952,
      "step": 1570
    },
    {
      "epoch": 1.9713038053649408,
      "grad_norm": 4.21875,
      "learning_rate": 3.42488555971702e-05,
      "loss": 0.8413,
      "step": 1580
    },
    {
      "epoch": 1.98378041172801,
      "grad_norm": 3.875,
      "learning_rate": 3.383270911360799e-05,
      "loss": 0.9076,
      "step": 1590
    },
    {
      "epoch": 1.9962570180910792,
      "grad_norm": 3.28125,
      "learning_rate": 3.341656263004578e-05,
      "loss": 0.8753,
      "step": 1600
    },
    {
      "epoch": 2.0087336244541483,
      "grad_norm": 3.359375,
      "learning_rate": 3.300041614648357e-05,
      "loss": 0.7521,
      "step": 1610
    },
    {
      "epoch": 2.0212102308172177,
      "grad_norm": 3.046875,
      "learning_rate": 3.258426966292135e-05,
      "loss": 0.7416,
      "step": 1620
    },
    {
      "epoch": 2.0336868371802868,
      "grad_norm": 3.453125,
      "learning_rate": 3.2168123179359136e-05,
      "loss": 0.7052,
      "step": 1630
    },
    {
      "epoch": 2.046163443543356,
      "grad_norm": 3.578125,
      "learning_rate": 3.175197669579692e-05,
      "loss": 0.622,
      "step": 1640
    },
    {
      "epoch": 2.0586400499064252,
      "grad_norm": 2.984375,
      "learning_rate": 3.1335830212234704e-05,
      "loss": 0.6595,
      "step": 1650
    },
    {
      "epoch": 2.0711166562694947,
      "grad_norm": 3.421875,
      "learning_rate": 3.0919683728672495e-05,
      "loss": 0.6854,
      "step": 1660
    },
    {
      "epoch": 2.083593262632564,
      "grad_norm": 3.046875,
      "learning_rate": 3.050353724511028e-05,
      "loss": 0.646,
      "step": 1670
    },
    {
      "epoch": 2.096069868995633,
      "grad_norm": 3.84375,
      "learning_rate": 3.0087390761548067e-05,
      "loss": 0.6905,
      "step": 1680
    },
    {
      "epoch": 2.108546475358702,
      "grad_norm": 3.890625,
      "learning_rate": 2.9671244277985854e-05,
      "loss": 0.7246,
      "step": 1690
    },
    {
      "epoch": 2.1210230817217717,
      "grad_norm": 3.09375,
      "learning_rate": 2.9255097794423635e-05,
      "loss": 0.641,
      "step": 1700
    },
    {
      "epoch": 2.133499688084841,
      "grad_norm": 2.78125,
      "learning_rate": 2.8838951310861422e-05,
      "loss": 0.7284,
      "step": 1710
    },
    {
      "epoch": 2.14597629444791,
      "grad_norm": 3.171875,
      "learning_rate": 2.842280482729921e-05,
      "loss": 0.699,
      "step": 1720
    },
    {
      "epoch": 2.158452900810979,
      "grad_norm": 3.6875,
      "learning_rate": 2.8006658343736998e-05,
      "loss": 0.6312,
      "step": 1730
    },
    {
      "epoch": 2.1709295071740486,
      "grad_norm": 4.28125,
      "learning_rate": 2.7590511860174785e-05,
      "loss": 0.6736,
      "step": 1740
    },
    {
      "epoch": 2.183406113537118,
      "grad_norm": 4.40625,
      "learning_rate": 2.717436537661257e-05,
      "loss": 0.6506,
      "step": 1750
    },
    {
      "epoch": 2.195882719900187,
      "grad_norm": 2.890625,
      "learning_rate": 2.6758218893050357e-05,
      "loss": 0.6617,
      "step": 1760
    },
    {
      "epoch": 2.2083593262632566,
      "grad_norm": 3.265625,
      "learning_rate": 2.6342072409488137e-05,
      "loss": 0.6401,
      "step": 1770
    },
    {
      "epoch": 2.2208359326263256,
      "grad_norm": 3.234375,
      "learning_rate": 2.5925925925925925e-05,
      "loss": 0.766,
      "step": 1780
    },
    {
      "epoch": 2.233312538989395,
      "grad_norm": 3.484375,
      "learning_rate": 2.5509779442363712e-05,
      "loss": 0.6404,
      "step": 1790
    },
    {
      "epoch": 2.245789145352464,
      "grad_norm": 3.375,
      "learning_rate": 2.50936329588015e-05,
      "loss": 0.643,
      "step": 1800
    },
    {
      "epoch": 2.2582657517155336,
      "grad_norm": 4.5,
      "learning_rate": 2.4677486475239287e-05,
      "loss": 0.6666,
      "step": 1810
    },
    {
      "epoch": 2.2707423580786026,
      "grad_norm": 3.375,
      "learning_rate": 2.426133999167707e-05,
      "loss": 0.6892,
      "step": 1820
    },
    {
      "epoch": 2.283218964441672,
      "grad_norm": 3.140625,
      "learning_rate": 2.384519350811486e-05,
      "loss": 0.6431,
      "step": 1830
    },
    {
      "epoch": 2.295695570804741,
      "grad_norm": 3.4375,
      "learning_rate": 2.3429047024552643e-05,
      "loss": 0.7046,
      "step": 1840
    },
    {
      "epoch": 2.3081721771678105,
      "grad_norm": 4.0,
      "learning_rate": 2.3012900540990427e-05,
      "loss": 0.6294,
      "step": 1850
    },
    {
      "epoch": 2.3206487835308796,
      "grad_norm": 3.03125,
      "learning_rate": 2.2596754057428215e-05,
      "loss": 0.6672,
      "step": 1860
    },
    {
      "epoch": 2.333125389893949,
      "grad_norm": 3.4375,
      "learning_rate": 2.2180607573866002e-05,
      "loss": 0.7307,
      "step": 1870
    },
    {
      "epoch": 2.345601996257018,
      "grad_norm": 3.5,
      "learning_rate": 2.176446109030379e-05,
      "loss": 0.7017,
      "step": 1880
    },
    {
      "epoch": 2.3580786026200875,
      "grad_norm": 3.921875,
      "learning_rate": 2.1348314606741574e-05,
      "loss": 0.6521,
      "step": 1890
    },
    {
      "epoch": 2.3705552089831565,
      "grad_norm": 4.5,
      "learning_rate": 2.093216812317936e-05,
      "loss": 0.6762,
      "step": 1900
    },
    {
      "epoch": 2.383031815346226,
      "grad_norm": 4.0625,
      "learning_rate": 2.0516021639617146e-05,
      "loss": 0.6685,
      "step": 1910
    },
    {
      "epoch": 2.395508421709295,
      "grad_norm": 3.765625,
      "learning_rate": 2.009987515605493e-05,
      "loss": 0.6192,
      "step": 1920
    },
    {
      "epoch": 2.4079850280723645,
      "grad_norm": 3.734375,
      "learning_rate": 1.9683728672492717e-05,
      "loss": 0.7099,
      "step": 1930
    },
    {
      "epoch": 2.4204616344354335,
      "grad_norm": 3.828125,
      "learning_rate": 1.9267582188930505e-05,
      "loss": 0.6209,
      "step": 1940
    },
    {
      "epoch": 2.432938240798503,
      "grad_norm": 3.46875,
      "learning_rate": 1.885143570536829e-05,
      "loss": 0.7064,
      "step": 1950
    },
    {
      "epoch": 2.445414847161572,
      "grad_norm": 3.3125,
      "learning_rate": 1.8435289221806076e-05,
      "loss": 0.6869,
      "step": 1960
    },
    {
      "epoch": 2.4578914535246414,
      "grad_norm": 3.796875,
      "learning_rate": 1.8019142738243864e-05,
      "loss": 0.707,
      "step": 1970
    },
    {
      "epoch": 2.4703680598877105,
      "grad_norm": 4.4375,
      "learning_rate": 1.760299625468165e-05,
      "loss": 0.6814,
      "step": 1980
    },
    {
      "epoch": 2.48284466625078,
      "grad_norm": 3.203125,
      "learning_rate": 1.7186849771119436e-05,
      "loss": 0.5892,
      "step": 1990
    },
    {
      "epoch": 2.495321272613849,
      "grad_norm": 4.5,
      "learning_rate": 1.677070328755722e-05,
      "loss": 0.7232,
      "step": 2000
    },
    {
      "epoch": 2.5077978789769184,
      "grad_norm": 4.40625,
      "learning_rate": 1.6354556803995007e-05,
      "loss": 0.7298,
      "step": 2010
    },
    {
      "epoch": 2.5202744853399874,
      "grad_norm": 4.3125,
      "learning_rate": 1.593841032043279e-05,
      "loss": 0.7463,
      "step": 2020
    },
    {
      "epoch": 2.532751091703057,
      "grad_norm": 3.8125,
      "learning_rate": 1.552226383687058e-05,
      "loss": 0.6892,
      "step": 2030
    },
    {
      "epoch": 2.545227698066126,
      "grad_norm": 3.75,
      "learning_rate": 1.5106117353308366e-05,
      "loss": 0.6593,
      "step": 2040
    },
    {
      "epoch": 2.5577043044291954,
      "grad_norm": 4.5,
      "learning_rate": 1.468997086974615e-05,
      "loss": 0.651,
      "step": 2050
    },
    {
      "epoch": 2.5701809107922644,
      "grad_norm": 3.03125,
      "learning_rate": 1.4273824386183936e-05,
      "loss": 0.6688,
      "step": 2060
    },
    {
      "epoch": 2.582657517155334,
      "grad_norm": 3.6875,
      "learning_rate": 1.3857677902621724e-05,
      "loss": 0.6665,
      "step": 2070
    },
    {
      "epoch": 2.595134123518403,
      "grad_norm": 3.765625,
      "learning_rate": 1.3441531419059511e-05,
      "loss": 0.6449,
      "step": 2080
    },
    {
      "epoch": 2.6076107298814724,
      "grad_norm": 3.484375,
      "learning_rate": 1.3025384935497296e-05,
      "loss": 0.692,
      "step": 2090
    },
    {
      "epoch": 2.6200873362445414,
      "grad_norm": 4.25,
      "learning_rate": 1.2609238451935081e-05,
      "loss": 0.6243,
      "step": 2100
    },
    {
      "epoch": 2.632563942607611,
      "grad_norm": 3.78125,
      "learning_rate": 1.2193091968372867e-05,
      "loss": 0.7056,
      "step": 2110
    },
    {
      "epoch": 2.64504054897068,
      "grad_norm": 3.359375,
      "learning_rate": 1.1776945484810655e-05,
      "loss": 0.6584,
      "step": 2120
    },
    {
      "epoch": 2.6575171553337493,
      "grad_norm": 3.625,
      "learning_rate": 1.136079900124844e-05,
      "loss": 0.653,
      "step": 2130
    },
    {
      "epoch": 2.6699937616968183,
      "grad_norm": 3.890625,
      "learning_rate": 1.0944652517686226e-05,
      "loss": 0.7296,
      "step": 2140
    },
    {
      "epoch": 2.682470368059888,
      "grad_norm": 3.9375,
      "learning_rate": 1.0528506034124012e-05,
      "loss": 0.6971,
      "step": 2150
    },
    {
      "epoch": 2.694946974422957,
      "grad_norm": 3.53125,
      "learning_rate": 1.0112359550561798e-05,
      "loss": 0.6742,
      "step": 2160
    },
    {
      "epoch": 2.7074235807860263,
      "grad_norm": 4.21875,
      "learning_rate": 9.696213066999585e-06,
      "loss": 0.6328,
      "step": 2170
    },
    {
      "epoch": 2.7199001871490953,
      "grad_norm": 3.609375,
      "learning_rate": 9.28006658343737e-06,
      "loss": 0.6875,
      "step": 2180
    },
    {
      "epoch": 2.732376793512165,
      "grad_norm": 3.890625,
      "learning_rate": 8.863920099875155e-06,
      "loss": 0.7429,
      "step": 2190
    },
    {
      "epoch": 2.744853399875234,
      "grad_norm": 3.25,
      "learning_rate": 8.447773616312943e-06,
      "loss": 0.7376,
      "step": 2200
    },
    {
      "epoch": 2.7573300062383033,
      "grad_norm": 4.28125,
      "learning_rate": 8.031627132750729e-06,
      "loss": 0.6727,
      "step": 2210
    },
    {
      "epoch": 2.7698066126013723,
      "grad_norm": 4.28125,
      "learning_rate": 7.615480649188515e-06,
      "loss": 0.6907,
      "step": 2220
    },
    {
      "epoch": 2.7822832189644418,
      "grad_norm": 4.0625,
      "learning_rate": 7.1993341656263e-06,
      "loss": 0.6634,
      "step": 2230
    },
    {
      "epoch": 2.7947598253275108,
      "grad_norm": 3.25,
      "learning_rate": 6.783187682064086e-06,
      "loss": 0.6445,
      "step": 2240
    },
    {
      "epoch": 2.8072364316905802,
      "grad_norm": 3.90625,
      "learning_rate": 6.367041198501873e-06,
      "loss": 0.6916,
      "step": 2250
    },
    {
      "epoch": 2.8197130380536493,
      "grad_norm": 3.46875,
      "learning_rate": 5.950894714939659e-06,
      "loss": 0.6982,
      "step": 2260
    },
    {
      "epoch": 2.8321896444167187,
      "grad_norm": 4.0625,
      "learning_rate": 5.534748231377445e-06,
      "loss": 0.6708,
      "step": 2270
    },
    {
      "epoch": 2.8446662507797877,
      "grad_norm": 3.140625,
      "learning_rate": 5.118601747815231e-06,
      "loss": 0.6742,
      "step": 2280
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 3.625,
      "learning_rate": 4.702455264253018e-06,
      "loss": 0.6517,
      "step": 2290
    },
    {
      "epoch": 2.8696194635059262,
      "grad_norm": 3.234375,
      "learning_rate": 4.286308780690804e-06,
      "loss": 0.7207,
      "step": 2300
    },
    {
      "epoch": 2.8820960698689957,
      "grad_norm": 3.828125,
      "learning_rate": 3.8701622971285895e-06,
      "loss": 0.6964,
      "step": 2310
    },
    {
      "epoch": 2.8945726762320647,
      "grad_norm": 3.703125,
      "learning_rate": 3.4540158135663753e-06,
      "loss": 0.7242,
      "step": 2320
    },
    {
      "epoch": 2.907049282595134,
      "grad_norm": 4.03125,
      "learning_rate": 3.0378693300041616e-06,
      "loss": 0.6499,
      "step": 2330
    },
    {
      "epoch": 2.919525888958203,
      "grad_norm": 4.0625,
      "learning_rate": 2.621722846441948e-06,
      "loss": 0.6749,
      "step": 2340
    },
    {
      "epoch": 2.9320024953212727,
      "grad_norm": 3.171875,
      "learning_rate": 2.2055763628797336e-06,
      "loss": 0.6573,
      "step": 2350
    },
    {
      "epoch": 2.944479101684342,
      "grad_norm": 3.453125,
      "learning_rate": 1.7894298793175199e-06,
      "loss": 0.6752,
      "step": 2360
    },
    {
      "epoch": 2.956955708047411,
      "grad_norm": 3.5625,
      "learning_rate": 1.373283395755306e-06,
      "loss": 0.7128,
      "step": 2370
    },
    {
      "epoch": 2.96943231441048,
      "grad_norm": 4.0625,
      "learning_rate": 9.57136912193092e-07,
      "loss": 0.6667,
      "step": 2380
    },
    {
      "epoch": 2.9819089207735496,
      "grad_norm": 3.75,
      "learning_rate": 5.409904286308781e-07,
      "loss": 0.6719,
      "step": 2390
    },
    {
      "epoch": 2.994385527136619,
      "grad_norm": 3.96875,
      "learning_rate": 1.2484394506866418e-07,
      "loss": 0.7079,
      "step": 2400
    }
  ],
  "logging_steps": 10,
  "max_steps": 2403,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.450465414315868e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}

import os

# æŒ‡å®šæ¨¡å‹ä¸‹è½½ç¼“å­˜è·¯å¾„ï¼Œä¸éœ€è¦è¯·æ³¨é‡Š
os.environ["HF_HOME"] = '/root/autodl-tmp/huggingface'
os.environ["TRANSFORMERS_CACHE"] = '/root/autodl-tmp/huggingface'
import torch
import streamlit as st
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

# åœ¨ä¾§è¾¹æ ä¸­åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªé“¾æ¥
with st.sidebar:
    st.markdown("## Qwen2.5 ä¿¡å®œè¯å¤§æ¨¡å‹")
    # åˆ›å»ºä¸€ä¸ªæ»‘å—ï¼Œç”¨äºé€‰æ‹©æœ€å¤§é•¿åº¦ï¼ŒèŒƒå›´åœ¨ 0 åˆ° 8192 ä¹‹é—´ï¼Œé»˜è®¤å€¼ä¸º 512ï¼ˆQwen2.5 æ”¯æŒ 128K ä¸Šä¸‹æ–‡ï¼Œå¹¶èƒ½ç”Ÿæˆæœ€å¤š 8K tokensï¼‰
    max_length = st.slider("max_length", 0, 8192, 2500, step=1)

# åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªå‰¯æ ‡é¢˜
st.title("ğŸ’¬ Qwen2.5 ä¿¡å®œè¯å¤§æ¨¡å‹")
st.caption("ğŸš€ A streamlit chatbot powered by Self-LLM")

# å®šä¹‰æ¨¡å‹è·¯å¾„
mode_name_or_path = "Qwen/Qwen2.5-7B-Instruct"


# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè·å–æ¨¡å‹å’Œ tokenizer
@st.cache_resource
def get_model():
    # ä»é¢„è®­ç»ƒçš„æ¨¡å‹ä¸­è·å– tokenizer
    tokenizer = AutoTokenizer.from_pretrained(mode_name_or_path, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token  # ç¡®ä¿ pad_token è®¾ç½®æ­£ç¡®

    # ä»é¢„è®­ç»ƒçš„æ¨¡å‹ä¸­è·å–æ¨¡å‹ï¼Œå¹¶è®¾ç½®æ¨¡å‹å‚æ•°
    model = AutoModelForCausalLM.from_pretrained(
        mode_name_or_path,
        torch_dtype=torch.float16,  # ä¿®æ”¹ä¸º float16 ä»¥ç¡®ä¿æ›´å¥½çš„å…¼å®¹æ€§
        device_map="auto"  # è‡ªåŠ¨æ˜ å°„åˆ°å¯ç”¨çš„è®¾å¤‡
    )

    # ç¡®ä¿ LoRA æƒé‡è·¯å¾„æ­£ç¡®
    lora_path = '/root/autodl-tmp/project/dialect_model/output/Qwen2.5_instruct_lora/checkpoint-2403/'
    if not os.path.exists(lora_path):
        raise ValueError(f"LoRA æƒé‡è·¯å¾„ä¸å­˜åœ¨: {lora_path}")

    # åŠ è½½ LoRA æƒé‡
    model = PeftModel.from_pretrained(model, model_id=lora_path, load_in_fp16=True)  # å¯å°è¯•åŠ è½½ 8bit æƒé‡

    return tokenizer, model


# åŠ è½½ Qwen2.5 çš„ model å’Œ tokenizer
tokenizer, model = get_model()

# å¦‚æœ session_state ä¸­æ²¡æœ‰ "messages"ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªåŒ…å«é»˜è®¤æ¶ˆæ¯çš„åˆ—è¡¨
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "æˆ‘ç³»è¯†è®²ä¿¡å®œè¯è‘›æ™ºèƒ½åŠ©æ‰‹é˜¿ä¿¡ã€‚ä½ å¯ä»¥åŒæˆ‘è®²ä¿¡å®œè¯ï¼Œäº¦å¯ä»¥åŒæˆ‘è®²æ™®é€šè¯ã€‚ä¸è¿‡æˆ‘è®­ç»ƒæ•°æ®çŸ›å‡ å¥½ï¼Œæ•ˆæœçŸ›å¥½çŸ›å¼æˆ‘ã€‚"}]

# éå† session_state ä¸­çš„æ‰€æœ‰æ¶ˆæ¯ï¼Œå¹¶æ˜¾ç¤ºåœ¨èŠå¤©ç•Œé¢ä¸Š
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# å¦‚æœç”¨æˆ·åœ¨èŠå¤©è¾“å…¥æ¡†ä¸­è¾“å…¥äº†å†…å®¹ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
if prompt := st.chat_input():
    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºç”¨æˆ·çš„è¾“å…¥
    st.chat_message("user").write(prompt)

    # å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ° session_state ä¸­çš„ messages åˆ—è¡¨ä¸­
    st.session_state.messages.append({"role": "user", "content": prompt})

    print(st.session_state.messages)
    # å°†å¯¹è¯è¾“å…¥æ¨¡å‹ï¼Œè·å¾—è¿”å›
    inputs = tokenizer.apply_chat_template(
        st.session_state.messages,
        add_generation_prompt=True,
        tokenize=True,
        return_tensors="pt",
        return_dict=True
    ).to(model.device)

    gen_kwargs = {
        "max_length": max_length,  # è°ƒæ•´ä¸ºä½ æœŸæœ›çš„é•¿åº¦
        "do_sample": True,  # å¯ç”¨é‡‡æ ·ä»¥è·å¾—æ›´å¤šæ ·çš„è¾“å‡º
        "top_k": 50,  # å¢å¤§top_kä»¥æé«˜å¤šæ ·æ€§
        "temperature": 0.9  # å¯è°ƒèŠ‚ç”Ÿæˆçš„å¤šæ ·æ€§ï¼Œ0.9ç›¸å¯¹è¾ƒä¸ºä¿å®ˆï¼Œ1.0æ›´è‡ªç”±
    }
    with torch.no_grad():
        outputs = model.generate(**inputs, **gen_kwargs)
        outputs = outputs[:, inputs['input_ids'].shape[1]:]
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # input_ids = tokenizer.encode(prompt, return_tensors="pt").to(model.device)  # ç›´æ¥ç¼–ç è¾“å…¥
    # generated_ids = model.generate(input_ids, max_new_tokens=max_length)  # ç”Ÿæˆå›ç­”
    #
    # # è§£ç æ¨¡å‹çš„è¾“å‡ºå¹¶å¤„ç†
    # response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)

    # å°†æ¨¡å‹çš„è¾“å‡ºæ·»åŠ åˆ° session_state ä¸­çš„ messages åˆ—è¡¨ä¸­
    st.session_state.messages.append({"role": "assistant", "content": response})

    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºæ¨¡å‹çš„è¾“å‡º
    st.chat_message("assistant").write(response)

    # æ‰“å° session_state è°ƒè¯•ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
    # print(st.session_state)
